{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Installation\n",
        "\n",
        "1. Run all code below to set up the environment\n",
        "2. Upload input data file to /content folder in Google colab repo"
      ],
      "metadata": {
        "id": "ihOHuiYRc9D-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6dELLWQMuJ52"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "c6LZRGBhulWs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.3.2-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "TTRPJDYIu-St"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\""
      ],
      "metadata": {
        "id": "Fz9bcACnvvAI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "L3Oyknj6wGLs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SPARK_HOME\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KA6Jv77EwKZp",
        "outputId": "358dcee8-d554-4bd7-8797-0c21d3e3b064"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.3.2-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code for Question 1\n",
        "\n",
        "Run Each cell sequentially.\n",
        "\n",
        "**Some cells might take 30 sec - 2 min to complete running since RDD follows lazy execution."
      ],
      "metadata": {
        "id": "GLodZ18AwUgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "from operator import add"
      ],
      "metadata": {
        "id": "bscU5vuuwTXC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file():\n",
        "  file1 = open(\"/content/soc-LiveJournal1Adj.txt\", \"r\")\n",
        "  friend_graph_dict = {}\n",
        "\n",
        "  temp = file1.readline()\n",
        "  while temp:\n",
        "    key,value = temp.split(\"\\t\")\n",
        "    friend_graph_dict[key] = value.replace(\"\\n\",\"\").split(\",\")\n",
        "    temp = file1.readline()\n",
        "  \n",
        "  file1.close()\n",
        "  return bfs_dict(friend_graph_dict)"
      ],
      "metadata": {
        "id": "CKcC_yA7yF1L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bfs_dict(grph):\n",
        "    \n",
        "    key_list = []\n",
        "\n",
        "    for node in grph.keys():\n",
        "      stack = []\n",
        "      exclude_list = set() \n",
        "      stack.append(node)\n",
        "      exclude_list.add(node)\n",
        "\n",
        "      for x in grph[node]:\n",
        "        stack.append(x)\n",
        "        exclude_list.add(x)\n",
        "\n",
        "      while stack:\n",
        "        root = stack[-1]\n",
        "        try:\n",
        "          for c in grph[root]:\n",
        "            if c not in exclude_list:\n",
        "              key_list.append((node, c))\n",
        "          stack.pop()\n",
        "        except KeyError:\n",
        "          break\n",
        "    \n",
        "        \n",
        "    return key_list"
      ],
      "metadata": {
        "id": "8CpGSQ29BFom"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommended_users(out_tuple, limit=10):\n",
        "\n",
        "  recommended_list = []\n",
        "  sorted_recc = sorted(out_tuple[1], key=lambda x:(x[1],1/int(x[0])),reverse=True)\n",
        "  if len(sorted_recc) <= limit:\n",
        "    recommended_list.append([out_tuple[0],sorted_recc])\n",
        "  else:\n",
        "    recommended_list.append([out_tuple[0],sorted_recc[0:10]]) \n",
        "\n",
        "  return recommended_list\n"
      ],
      "metadata": {
        "id": "MtGJqNQmnUMF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the Spark Session\n",
        "sc1 = SparkContext(\"local\", \"FriendRecommendation\")"
      ],
      "metadata": {
        "id": "i2Siyljv6jQW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess file and create Map RDD\n",
        "dataset = process_file()\n",
        "rdd1 = sc1.parallelize(dataset,100)\n",
        "\n",
        "map_rdd = rdd1.map(lambda x: (x,1))"
      ],
      "metadata": {
        "id": "kvj-9X4P7ryN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shuffle/sort RDD\n",
        "shuffle_rdd = map_rdd.sortByKey()\n",
        "print(shuffle_rdd.take(10))\n",
        "print(shuffle_rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCiZSXw28g-9",
        "outputId": "e9d560de-f7c5-4423-a781-6c175de3a7e1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('0', '10001'), 1), (('0', '1001'), 1), (('0', '10014'), 1), (('0', '10018'), 1), (('0', '10020'), 1), (('0', '10023'), 1), (('0', '10025'), 1), (('0', '10038'), 1), (('0', '10041'), 1), (('0', '10042'), 1)]\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reduce\n",
        "counts = shuffle_rdd.reduceByKey(add, 100)\n",
        "print(counts.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8xlfl6F861c",
        "outputId": "7ad59204-9e59-4102-e666-f281c0d0e4b5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(counts.collect()))\n",
        "#print(counts.take(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiQ56iKnDnDK",
        "outputId": "2b28e01c-d923-4b95-e8ed-335138094922"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12054196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make groups based on user_id\n",
        "map_rdd2 = counts.map(lambda x: (x[0][0], (x[0][1],x[1])))"
      ],
      "metadata": {
        "id": "TAtKLKptAnkz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_rdd = map_rdd2.groupByKey(500)"
      ],
      "metadata": {
        "id": "gvfuXkJuCVkC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(group_rdd.take(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j1Gnw65Cqhm",
        "outputId": "e1381efc-dab3-449d-cb6c-3aff0ec202c8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('10513', <pyspark.resultiterable.ResultIterable object at 0x7f26746274f0>), ('1083', <pyspark.resultiterable.ResultIterable object at 0x7f26746279d0>), ('1114', <pyspark.resultiterable.ResultIterable object at 0x7f267d03c9a0>), ('11190', <pyspark.resultiterable.ResultIterable object at 0x7f267d03ceb0>), ('11779', <pyspark.resultiterable.ResultIterable object at 0x7f267d03c430>), ('13035', <pyspark.resultiterable.ResultIterable object at 0x7f267d03ce50>), ('13516', <pyspark.resultiterable.ResultIterable object at 0x7f267460e7f0>), ('13977', <pyspark.resultiterable.ResultIterable object at 0x7f267460e670>), ('14121', <pyspark.resultiterable.ResultIterable object at 0x7f267460e4c0>), ('15523', <pyspark.resultiterable.ResultIterable object at 0x7f26e89fdc40>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query Users"
      ],
      "metadata": {
        "id": "WKReS90-0IUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Querying User Recommendations\n",
        "user_list = [924,8941,8942,9019,9020,9021,9022,9990,9992,9993]\n",
        "user_list = list(map(str, [x for x in user_list]))\n",
        "\n",
        "data = group_rdd.filter(lambda x:(x[0]in user_list)) \n",
        "print(data.take(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te8aolC4EBRh",
        "outputId": "42b1bb3c-df66-499e-f7a6-b109a0e559fa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('8941', <pyspark.resultiterable.ResultIterable object at 0x7f2674627f70>), ('9022', <pyspark.resultiterable.ResultIterable object at 0x7f2674627f40>), ('9020', <pyspark.resultiterable.ResultIterable object at 0x7f26746276a0>), ('9021', <pyspark.resultiterable.ResultIterable object at 0x7f2674627c10>), ('924', <pyspark.resultiterable.ResultIterable object at 0x7f2674627d30>), ('9993', <pyspark.resultiterable.ResultIterable object at 0x7f2674627a60>), ('9990', <pyspark.resultiterable.ResultIterable object at 0x7f2674627610>), ('9992', <pyspark.resultiterable.ResultIterable object at 0x7f2674627d00>), ('9019', <pyspark.resultiterable.ResultIterable object at 0x7f26746279d0>), ('8942', <pyspark.resultiterable.ResultIterable object at 0x7f2674627670>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top10 = sorted(data.flatMap(get_recommended_users).collect(), key=lambda x: int(x[0]))"
      ],
      "metadata": {
        "id": "ec4LacbvE6VU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Print Results"
      ],
      "metadata": {
        "id": "zG4QdpFndyJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print (user_id, [(recommended_user, number_mutual_friends)])\n",
        "print(*top10,sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QJ8deySj9oY",
        "outputId": "517cb238-680c-4366-cda0-3c3e13cdbc93"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['924', [('439', 1), ('2409', 1), ('6995', 1), ('11860', 1), ('15416', 1), ('43748', 1), ('45881', 1)]]\n",
            "['8941', [('8943', 2), ('8944', 2), ('8940', 1)]]\n",
            "['8942', [('8939', 3), ('8940', 1), ('8943', 1), ('8944', 1)]]\n",
            "['9019', [('9022', 2), ('317', 1), ('9023', 1)]]\n",
            "['9020', [('9021', 3), ('9016', 2), ('9017', 2), ('9022', 2), ('317', 1), ('9023', 1)]]\n",
            "['9021', [('9020', 3), ('9016', 2), ('9017', 2), ('9022', 2), ('317', 1), ('9023', 1)]]\n",
            "['9022', [('9019', 2), ('9020', 2), ('9021', 2), ('317', 1), ('9016', 1), ('9017', 1), ('9023', 1)]]\n",
            "['9990', [('13134', 1), ('13478', 1), ('13877', 1), ('34299', 1), ('34485', 1), ('34642', 1), ('37941', 1)]]\n",
            "['9992', [('9987', 4), ('9989', 4), ('35667', 3), ('9991', 2)]]\n",
            "['9993', [('9991', 5), ('13134', 1), ('13478', 1), ('13877', 1), ('34299', 1), ('34485', 1), ('34642', 1), ('37941', 1)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print Recommendations in required format\n",
        "file2 = open(\"top10_results\", \"w\")\n",
        "for line in top10:\n",
        "\n",
        "  temp_str = line[0]+\"\\t\"\n",
        "  for i in range(0,len(line[1])):\n",
        "    if i == len(line[1])-1:\n",
        "      temp_str += line[1][i][0]+\"\\n\"\n",
        "    else:\n",
        "      temp_str += line[1][i][0]+\", \"\n",
        "  file2.write(temp_str)\n",
        "  print(temp_str, end=\"\")\n",
        "file2.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ek_IUW5FcDu",
        "outputId": "92fc3c2d-a841-496c-8e79-7f79239b36a8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "924\t439, 2409, 6995, 11860, 15416, 43748, 45881\n",
            "8941\t8943, 8944, 8940\n",
            "8942\t8939, 8940, 8943, 8944\n",
            "9019\t9022, 317, 9023\n",
            "9020\t9021, 9016, 9017, 9022, 317, 9023\n",
            "9021\t9020, 9016, 9017, 9022, 317, 9023\n",
            "9022\t9019, 9020, 9021, 317, 9016, 9017, 9023\n",
            "9990\t13134, 13478, 13877, 34299, 34485, 34642, 37941\n",
            "9992\t9987, 9989, 35667, 9991\n",
            "9993\t9991, 13134, 13478, 13877, 34299, 34485, 34642, 37941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stop the SparkContext object\n",
        "sc1.stop()"
      ],
      "metadata": {
        "id": "Kol95igMlWby"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}
